import argparse
import logging
import re

from concurrent.futures.thread import ThreadPoolExecutor
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple, Union
from tqdm.auto import tqdm

from lhotse.utils import Pathlike
from lhotse import fix_manifests, validate_recordings_and_supervisions
from lhotse.audio import Recording, RecordingSet
from lhotse.recipes.utils import manifests_exist, read_manifests_if_cached
from lhotse.supervision import SupervisionSegment, SupervisionSet
from lhotse.utils import Pathlike

VIETASR = (
    "dev",
    "test",
    "train",
)


def prepare_manifest(
    corpus_dir: Pathlike,
    language = "en",
    output_dir: Optional[Pathlike] = None,
    normalize_text: str = "none",
    num_jobs: int = 1,
) -> Dict[str, Dict[str, Union[RecordingSet, SupervisionSet]]]:
    """
    Returns the manifests which consist of the Recordings and Supervisions.
    When all the manifests are available in the ``output_dir``, it will simply read and return them.

    :param corpus_dir: Pathlike, the path of the data dir.
    :param dataset_parts: string or sequence of strings representing dataset part names, e.g. 'train-clean-100', 'train-clean-5', 'dev-clean'.
        By default we will infer which parts are available in ``corpus_dir``.
    :param output_dir: Pathlike, the path where to write the manifests.
    :param normalize_text: str, "none" or "lower",
        for "lower" the transcripts are converted to lower-case.
    :param num_jobs: int, number of parallel threads used for 'parse_utterance' calls.
    :return: a Dict whose key is the dataset part, and the value is Dicts with the keys 'audio' and 'supervisions'.
    """
    corpus_dir = Path(corpus_dir)
    assert corpus_dir.is_dir(), f"No such directory: {corpus_dir}"

    dataset_parts = (
        set(VIETASR)
        .intersection(path.name for path in corpus_dir.glob("*"))
    )
    if not dataset_parts:
        raise ValueError(
            f"Could not find any of splits in: {corpus_dir}"
        )

    manifests = {}

    if output_dir is not None:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        # Maybe the manifests already exist: we can read them and save a bit of preparation time.
        manifests = read_manifests_if_cached(
            dataset_parts=dataset_parts, output_dir=output_dir
        )

    with ThreadPoolExecutor(num_jobs) as ex:
        for part in tqdm(dataset_parts, desc="Dataset parts"):
            logging.info(f"Processing subset: {part}")
            if manifests_exist(part=part, output_dir=output_dir, prefix="vietASR"):
                logging.info(f"Subset: {part} already prepared - skipping.")
                continue
            recordings = []
            supervisions = []
            part_path = corpus_dir / part
            futures = []
            for trans_path in tqdm(
                part_path.rglob("*.trans.txt"), desc="Distributing tasks", leave=False
            ):
                # "trans_path" file contains lines like:
                #
                #   121-121726-0000 ALSO A POPULAR CONTRIVANCE
                #   121-121726-0001 HARANGUE THE TIRESOME PRODUCT OF A TIRELESS TONGUE
                #   121-121726-0002 ANGOR PAIN PAINFUL TO HEAR
                #
                # We will create a separate Recording and SupervisionSegment for those.
                with open(trans_path) as f:
                    for line in f:
                        futures.append(
                            ex.submit(parse_utterance, trans_path, line, language)
                        )

            for future in tqdm(futures, desc="Processing", leave=False):
                result = future.result()
                if result is None:
                    continue
                recording, segment = result
                recordings.append(recording)
                supervisions.append(segment)

            recording_set = RecordingSet.from_recordings(recordings)
            supervision_set = SupervisionSet.from_segments(supervisions)

            # Normalize text to lowercase
            if normalize_text == "lower":
                to_lower = lambda text: text.lower()
                supervision_set = SupervisionSet.from_segments(
                    [s.transform_text(to_lower) for s in supervision_set]
                )

            recording_set, supervision_set = fix_manifests(
                recording_set, supervision_set
            )
            validate_recordings_and_supervisions(recording_set, supervision_set)

            if output_dir is not None:
                supervision_set.to_file(
                    output_dir / f"vietASR_supervisions_{part}.jsonl.gz"
                )
                recording_set.to_file(
                    output_dir / f"vietASR_recordings_{part}.jsonl.gz"
                )

            manifests[part] = {
                "recordings": recording_set,
                "supervisions": supervision_set,
            }

    return manifests


def parse_utterance(
    script_path: Path,
    line: str,
    language: str,
) -> Optional[Tuple[Recording, SupervisionSegment]]:
    recording_id, text = line.strip().split(maxsplit=1)
    # Create the Recording first
    audio_path = (
        script_path.parent
        / f"{recording_id}.wav"
    )
    if not audio_path.is_file():
        logging.warning(f"No such file: {audio_path}")
        return None
    recording = Recording.from_file(audio_path, recording_id=recording_id)
    # Then, create the corresponding supervisions
    segment = SupervisionSegment(
        id=recording_id,
        recording_id=recording_id,
        start=0.0,
        duration=recording.duration,
        channel=0,
        language=language,
        speaker=re.sub(r"-.*", r"", recording.id),
        text=text.strip(),
    )
    return recording, segment

def run(
    corpus_dir: Pathlike,
    output_dir: Pathlike,
    lanugage: str,
    normalize_text: str,
    num_jobs: int,
):
    prepare_manifest(
        corpus_dir,
        output_dir=output_dir,
        language=lanugage,
        num_jobs=num_jobs,
        normalize_text=normalize_text,
    )

if __name__=="__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--corpus-dir", type=Path, help="Path to the data dir.")
    parser.add_argument("--output-dir", type=Path, help="Path where to write the manifests.")
    parser.add_argument("--language", type=str, help="dataset language")
    parser.add_argument("--normalize-text", type=str, help="Conversion of transcripts to lower-case (originally in upper-case)")
    parser.add_argument("--num-jobs", type=int, default=1, help="How many threads to use (can give good speed-ups with slow disks).")
    args = parser.parse_args()

    run(
        args.corpus_dir, 
        args.output_dir, 
        args.language, 
        args.normalize_text, 
        args.num_jobs
    )
    

